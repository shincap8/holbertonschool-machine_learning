# 0x0F. Natural Language Processing - Word Embeddings

### Description

This project is about implementing natural language processing - word embeddings.

### General Objectives

* What is natural language processing?
* What is a word embedding?
* What is bag of words?
* What is TF-IDF?
* What is CBOW?
* What is a skip-gram?
* What is an n-gram?
* What is negative sampling?
* What is word2vec, GloVe, fastText, ELMo?

### Mandatory Tasks

| File | Description |
| ------ | ------ |
| [0-bag_of_words.py](0-bag_of_words.py) | Creates a bag of words embedding matrix. |
| [1-tf_idf.py](1-tf_idf.py) | Creates a TF-IDF embedding. |
| [2-word2vec.py](2-word2vec.py) | Creates and trains a gensim word2vec model. |
| [3-gensim_to_keras.py](3-gensim_to_keras.py) | Converts a gensim word2vec model to a keras Embedding layer. |
| [4-fasttext.py](4-fasttext.py) | Creates and trains a genism fastText model. |
| [5-elmo](5-elmo) | When training an ELMo embedding model, you are training: The weights applied to the hidden states. |
